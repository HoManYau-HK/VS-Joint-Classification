{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a11ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0: Create environment\n",
    "! pip install graphviz\n",
    "! pip install topologicpy --upgrade\n",
    "\n",
    "import topologicpy\n",
    "\n",
    "from topologicpy.Vertex import Vertex\n",
    "from topologicpy.Edge import Edge\n",
    "from topologicpy.Wire import Wire\n",
    "from topologicpy.Face import Face\n",
    "from topologicpy.Shell import Shell\n",
    "from topologicpy.Cell import Cell\n",
    "from topologicpy.CellComplex import CellComplex\n",
    "from topologicpy.Cluster import Cluster\n",
    "from topologicpy.Graph import Graph\n",
    "from topologicpy.Topology import Topology\n",
    "from topologicpy.Dictionary import Dictionary\n",
    "from topologicpy.Matrix import Matrix\n",
    "from topologicpy.Helper import Helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22151ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from topologicpy.Topology import Topology\n",
    "from pathlib import Path\n",
    "\n",
    "def _is_break(s: str) -> bool:\n",
    "    return isinstance(s, str) and s.strip().lower() in {\"break\", \"q\", \"quit\", \"exit\"}\n",
    "\n",
    "def pick_obj_folder() -> Path:\n",
    "    try:\n",
    "        import tkinter as tk\n",
    "        from tkinter import filedialog\n",
    "        tk.Tk().withdraw()\n",
    "        chosen = filedialog.askdirectory(title=\"Select folder containing OBJ files\")\n",
    "        if chosen:\n",
    "            p = Path(chosen)\n",
    "            if p.is_dir():\n",
    "                return p\n",
    "    except Exception:\n",
    "        pass\n",
    "    while True:\n",
    "        raw = input('Enter path to folder containing OBJ files (or type \"break\" to cancel): ').strip().strip('\"')\n",
    "        if _is_break(raw):\n",
    "            raise KeyboardInterrupt(\"User cancelled folder selection.\")\n",
    "        p = Path(raw)\n",
    "        if p.is_dir():\n",
    "            return p\n",
    "        print(\"❗ Not a valid folder. Please try again.\")\n",
    "\n",
    "# Ask at runtime instead of hard-coding:\n",
    "OBJ_DIR = pick_obj_folder()\n",
    "print(f\"[Folder] Using OBJ directory: {OBJ_DIR}\")\n",
    "\n",
    "def import_obj_files(obj_dir):\n",
    "    # Collect and sort .obj files (case-insensitive) for stable ordering\n",
    "    obj_files = sorted([f for f in os.listdir(obj_dir) if f.lower().endswith('.obj')], key=str.lower)\n",
    "\n",
    "    topologies = []\n",
    "    for i, obj_file in enumerate(obj_files):\n",
    "        obj_path = os.path.join(obj_dir, obj_file)\n",
    "        topology = Topology.ByOBJPath(\n",
    "            objPath=obj_path,\n",
    "            defaultColor=[255, 255, 255],\n",
    "            defaultOpacity=0.5,\n",
    "            transposeAxes=True,\n",
    "            removeCoplanarFaces=False,\n",
    "            selfMerge=False,\n",
    "            mantissa=6,\n",
    "            tolerance=0.0001\n",
    "        )\n",
    "        topologies.append(topology)\n",
    "        globals()[f\"model_{i+1}\"] = topology\n",
    "        # Optional: echo each import line with index prefix as well\n",
    "        pad = max(2, len(str(len(obj_files))))\n",
    "        print(f\"[{(i+1):0{pad}d}] Imported: {obj_file} as model_{i+1}\")\n",
    "\n",
    "    # Final summary with [01], [02], [03] ... prefix\n",
    "    print(\"OBJ models imported:\")\n",
    "    pad = max(2, len(str(len(obj_files))))\n",
    "    for i, obj_file in enumerate(obj_files):\n",
    "        print(f\"[{(i+1):0{pad}d}] model_{i+1} = {obj_file}\")\n",
    "    return topologies\n",
    "\n",
    "def watch_obj_directory(obj_dir, interval=5):\n",
    "    seen_files = set()\n",
    "    while True:\n",
    "        current_files = set(f for f in os.listdir(obj_dir) if f.lower().endswith('.obj'))\n",
    "        new_files = current_files - seen_files\n",
    "        if new_files:\n",
    "            print(\"New OBJ files detected. Importing...\")\n",
    "            import_obj_files(obj_dir)\n",
    "            seen_files = current_files\n",
    "        elif current_files == seen_files and len(current_files) > 0:\n",
    "            print(\"All OBJ files have been uploaded. Stopping watcher.\")\n",
    "            break\n",
    "        time.sleep(interval)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Watching OBJ directory for new files...\")\n",
    "    watch_obj_directory(OBJ_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Root folder where all exports go\n",
    "EXPORT_ROOT = r\"C:\\Users\\user\\Downloads\\Topologic_Graph_Exports\"\n",
    "\n",
    "def _ensure_dir(path: str) -> str:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def _make_new_export_dir(root: str = EXPORT_ROOT, prefix: str = \"run_\") -> str:\n",
    "    _ensure_dir(root)\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")  # microsecond to avoid collisions\n",
    "    new_dir = os.path.join(root, f\"{prefix}{stamp}\")\n",
    "    os.makedirs(new_dir, exist_ok=False)\n",
    "    return new_dir\n",
    "\n",
    "# Initial export folder for the first CSV export of this session\n",
    "current_export_dir = _make_new_export_dir()\n",
    "print(f\"[Init] First export folder: {current_export_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display imported models and group selected ones\n",
    "print(\"Imported OBJ models:\")\n",
    "obj_files = [f for f in os.listdir(OBJ_DIR) if f.lower().endswith(\".obj\")]\n",
    "model_vars = sorted(\n",
    "    [n for n in globals() if n.startswith(\"model_\") and n.split(\"_\")[1].isdigit()],\n",
    "    key=lambda n: int(n.split(\"_\")[1]),\n",
    ")\n",
    "model_count = max(len(model_vars), len(obj_files))\n",
    "\n",
    "for i in range(1, model_count + 1):\n",
    "    fname = obj_files[i - 1] if i - 1 < len(obj_files) else \"(no corresponding OBJ file)\"\n",
    "    print(f\"[{i:02d}] model_{i} = {fname}\")\n",
    "\n",
    "selected = input(\"Enter the model numbers to group (comma-separated, e.g., 1,3,5): \")\n",
    "idx = sorted({int(s) for s in selected.replace(\" \", \"\").split(\",\") if s.isdigit() and 1 <= int(s) <= model_count})\n",
    "names = [f\"model_{i}\" for i in idx]\n",
    "\n",
    "grouped = [globals()[n] for n in names if n in globals()]\n",
    "missing = [n for n in names if n not in globals()]\n",
    "group_name = input(\"Enter a name for this group: \").strip()\n",
    "\n",
    "groups = globals().get(\"groups\", {})\n",
    "groups[group_name] = names\n",
    "globals()[\"groups\"] = groups\n",
    "globals()[group_name] = grouped\n",
    "\n",
    "print(f\"Grouped models as a list named '{group_name}':\\n{grouped}\")\n",
    "if missing:\n",
    "    print(\"Note: These variables were not found and were skipped:\", \", \".join(missing))\n",
    "\n",
    "Topology.Show(globals()[group_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36616ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each cluster in the grouped_models list and process them\n",
    "models = []\n",
    "for group in grouped:\n",
    "    for cluster in group:\n",
    "        faces = Topology.Faces(cluster)\n",
    "        faces = Helper.Flatten(faces)\n",
    "        model = Cell.ByFaces(faces)\n",
    "        models.append(model)\n",
    "\n",
    "print(\"List of models created from grouped_models:\")\n",
    "print(models)\n",
    "\n",
    "Topology.Show(models) #add [0/1] after models to see ther spesific model\n",
    "\n",
    "a1 = Topology.IsInstance(models[1], type=\"Cell\") # checking if the first object is an instance of Cell\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = CellComplex. ByCells(models)\n",
    "Topology.Show(models)\n",
    "print(models) #printing the object\n",
    "\n",
    "cells = Topology.Cells(models)\n",
    "if cells is None:\n",
    "\tprint(\"No cells found.\")\n",
    "else:\n",
    "\tprint(len(cells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cells)):\n",
    "    value = input(f\"Enter the value for cell {i} (key is 'type'): \")\n",
    "    d = Dictionary.ByKeyValue(\"type\", value)\n",
    "    cells[i] = Topology.SetDictionary(cells[i], d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78777dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = Topology.Faces(models)\n",
    "for face in faces:\n",
    "    parents = Topology.SuperTopologies(face, hostTopology=models, topologyType=\"cell\")\n",
    "    if len(parents) == 2:\n",
    "        d1 = Topology.Dictionary(parents[0])\n",
    "        d2 = Topology.Dictionary(parents[1])\n",
    "        value1 = Dictionary.ValueAtKey(d1, \"type\")\n",
    "        value2 = Dictionary.ValueAtKey(d2, \"type\")\n",
    "        value = value1+\"_\"+value2\n",
    "        d3 = Dictionary.ByKeyValue(\"type\", value)\n",
    "        face = Topology.SetDictionary(face, d3)\n",
    "    else:\n",
    "        d3 = Dictionary.ByKeyValue(\"type\", \"exterior_face\")\n",
    "        face = Topology.SetDictionary(face, d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d151a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user for boolean input for viaSharedTopologies and toExteriorTopologies\n",
    "via_shared = input(\"Enter True or False for viaSharedTopologies: \").strip().lower() == \"true\"\n",
    "to_exterior = input(\"Enter True or False for toExteriorTopologies: \").strip().lower() == \"true\"\n",
    "\n",
    "g = Graph.ByTopology(models, viaSharedTopologies=via_shared, toExteriorTopologies=to_exterior)\n",
    "Topology.Show(g, models)\n",
    "\n",
    "verts = Graph.Vertices(g)\n",
    "print(len(verts))\n",
    "for v in verts:\n",
    "    d = Topology.Dictionary(v)\n",
    "    print(Dictionary.Keys(d), Dictionary.Values(d))\n",
    "\n",
    "flat_g = Graph.Reshape(g)\n",
    "Topology.Show(flat_g)\n",
    "\n",
    "#Graph.ExportToCSV(g, (r\"C:\\Users\\user\\Downloads\\test_1\"), 0, overwrite=True, graphLabelHeader=\"label_id\", edgeLabelHeader=\"label_id\", nodeLabelHeader=\"label_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d462e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_break(s: str) -> bool:\n",
    "    return isinstance(s, str) and s.strip().lower() in {\"break\", \"q\", \"quit\", \"exit\"}\n",
    "\n",
    "if \"groups\" not in globals():\n",
    "    groups = {}\n",
    "\n",
    "def get_ungrouped_models(model_count, groups):\n",
    "    grouped_indices = set()\n",
    "    for v in groups.values():\n",
    "        for m in v:\n",
    "            idx = int(m.split(\"_\")[1])\n",
    "            grouped_indices.add(idx)\n",
    "    return [i for i in range(1, model_count+1) if i not in grouped_indices]\n",
    "\n",
    "def _cleanup_orphans(obj_files, groups):\n",
    "    \"\"\"\n",
    "    Remove globals model_<n> that don't have a corresponding OBJ file *by index*\n",
    "    and also scrub them from `groups`. This keeps state consistent.\n",
    "    \"\"\"\n",
    "    max_idx = len(obj_files)\n",
    "    removed = []\n",
    "\n",
    "    # 1) Remove any model_<n> where n exceeds current file count OR is None\n",
    "    for name in list(globals()):\n",
    "        if name.startswith(\"model_\"):\n",
    "            try:\n",
    "                idx = int(name.split(\"_\")[1])\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "            if idx > max_idx or globals()[name] is None:\n",
    "                globals().pop(name, None)\n",
    "                removed.append(idx)\n",
    "\n",
    "    # 2) Remove any model_<n> entries from groups that were purged\n",
    "    if removed:\n",
    "        removed_set = {f\"model_{i}\" for i in removed}\n",
    "        for k in list(groups.keys()):\n",
    "            groups[k] = [m for m in groups[k] if m not in removed_set]\n",
    "            if not groups[k]:\n",
    "                # drop empty group\n",
    "                groups.pop(k, None)\n",
    "\n",
    "    return sorted(removed)\n",
    "\n",
    "# ===================== MAIN LOOP =====================\n",
    "while True:\n",
    "    print(\"\\nImported OBJ models:\")\n",
    "    obj_files = [f for f in os.listdir(OBJ_DIR) if f.lower().endswith('.obj')]\n",
    "\n",
    "    # NEW: purge orphaned models that have no corresponding OBJ file\n",
    "    purged = _cleanup_orphans(obj_files, groups)\n",
    "    if purged:\n",
    "        print(f\"Removed models without OBJ: {[f'model_{i}' for i in purged]}\")\n",
    "\n",
    "    model_count = len([name for name in globals() if name.startswith(\"model_\")])\n",
    "\n",
    "    # Get ungrouped model indices, then filter to only those with an OBJ\n",
    "    ungrouped = [i for i in get_ungrouped_models(model_count, groups) if i-1 < len(obj_files)]\n",
    "    if not ungrouped:\n",
    "        print(\"All models have been grouped and processed.\")\n",
    "        break\n",
    "\n",
    "    for idx in ungrouped:\n",
    "        print(f\"[{idx:02d}] model_{idx} = {obj_files[idx-1]}\")\n",
    "\n",
    "    # ---- selection prompt (supports 'break') ----\n",
    "    selected_raw = input(\"Enter the model numbers to group (comma-separated, e.g., 1,3,5) or type 'break' to stop: \").strip()\n",
    "    if _is_break(selected_raw):\n",
    "        print(\"Stopping by user request.\")\n",
    "        break\n",
    "\n",
    "    selected_indices = []\n",
    "    for x in selected_raw.split(\",\"):\n",
    "        x = x.strip()\n",
    "        if x.isdigit():\n",
    "            xi = int(x)\n",
    "            if (xi in ungrouped) and (xi not in selected_indices):\n",
    "                selected_indices.append(xi)\n",
    "\n",
    "    if not selected_indices:\n",
    "        print(\"No valid ungrouped model numbers selected. Try again.\")\n",
    "        continue\n",
    "\n",
    "    grouped_models = [globals()[f\"model_{i}\"] for i in selected_indices]\n",
    "\n",
    "    # ---- group name prompt (supports 'break') ----\n",
    "    group_name = input(\"Enter a name for this group (or type 'break' to stop): \").strip()\n",
    "    if _is_break(group_name):\n",
    "        print(\"Stopping by user request.\")\n",
    "        break\n",
    "\n",
    "    groups[group_name] = [f\"model_{i}\" for i in selected_indices]\n",
    "    print(f\"Grouped models as a list named '{group_name}':\")\n",
    "    print(grouped_models)\n",
    "    globals()[group_name] = grouped_models\n",
    "\n",
    "\n",
    "    # ---- Process grouped_models as in previous cells ----\n",
    "    STOP_ALL = False  # sentinel to exit outer loop if user types 'break' mid-process\n",
    "\n",
    "    models = []\n",
    "    for group in grouped_models:\n",
    "        for cluster in group:\n",
    "            faces = Topology.Faces(cluster)\n",
    "            faces = Helper.Flatten(faces)\n",
    "            model = Cell.ByFaces(faces)\n",
    "            models.append(model)\n",
    "\n",
    "    models = CellComplex.ByCells(models)\n",
    "    cells = Topology.Cells(models)\n",
    "    if cells is None:\n",
    "        print(\"No cells found.\")\n",
    "        continue\n",
    "\n",
    "    # ---- per-cell dictionary assignment (supports 'break') ----\n",
    "    for i in range(len(cells)):\n",
    "        value = input(f\"Enter the value for cell {i} (key is 'type') or type 'break' to stop: \").strip()\n",
    "        if _is_break(value):\n",
    "            print(\"Stopping by user request.\")\n",
    "            STOP_ALL = True\n",
    "            break\n",
    "        d = Dictionary.ByKeyValue(\"type\", value)\n",
    "        cells[i] = Topology.SetDictionary(cells[i], d)\n",
    "\n",
    "    if STOP_ALL:\n",
    "        break\n",
    "\n",
    "    # ---- face labelling ----\n",
    "    faces = Topology.Faces(models)\n",
    "    for face in faces:\n",
    "        parents = Topology.SuperTopologies(face, hostTopology=models, topologyType=\"cell\")\n",
    "        if len(parents) == 2:\n",
    "            d1 = Topology.Dictionary(parents[0])\n",
    "            d2 = Topology.Dictionary(parents[1])\n",
    "            value1 = Dictionary.ValueAtKey(d1, \"type\")\n",
    "            value2 = Dictionary.ValueAtKey(d2, \"type\")\n",
    "            value = f\"{value1}_{value2}\"\n",
    "            d3 = Dictionary.ByKeyValue(\"type\", value)\n",
    "            face = Topology.SetDictionary(face, d3)\n",
    "        else:\n",
    "            d3 = Dictionary.ByKeyValue(\"type\", \"exterior_face\")\n",
    "            face = Topology.SetDictionary(face, d3)\n",
    "\n",
    "    # ---- graph flags (support 'break') ----\n",
    "    via_shared_raw = input(\"Enter True or False for viaSharedTopologies (or type 'break' to stop): \").strip()\n",
    "    if _is_break(via_shared_raw):\n",
    "        print(\"Stopping by user request.\")\n",
    "        break\n",
    "    to_exterior_raw = input(\"Enter True or False for toExteriorTopologies (or type 'break' to stop): \").strip()\n",
    "    if _is_break(to_exterior_raw):\n",
    "        print(\"Stopping by user request.\")\n",
    "        break\n",
    "\n",
    "    via_shared = via_shared_raw.lower() == \"true\"\n",
    "    to_exterior = to_exterior_raw.lower() == \"true\"\n",
    "\n",
    "    g = Graph.ByTopology(models, viaSharedTopologies=via_shared, toExteriorTopologies=to_exterior)\n",
    "    Topology.Show(g, models)\n",
    "    print(len(verts))\n",
    "    for v in verts:\n",
    "        d = Topology.Dictionary(v)\n",
    "        print(Dictionary.Keys(d), Dictionary.Values(d))\n",
    "\n",
    "    flat_g = Graph.Reshape(g)\n",
    "    Topology.Show(flat_g)\n",
    "    # ---- export CSV to the current folder ----\n",
    "_graph_export_path = _ensure_dir(current_export_dir)\n",
    "Graph.ExportToCSV(\n",
    "    g,\n",
    "    _graph_export_path,\n",
    "    0,\n",
    "    overwrite=True,\n",
    "    graphLabelHeader=\"label_id\",\n",
    "    edgeLabelHeader=\"label_id\",\n",
    "    nodeLabelHeader=\"label_id\"\n",
    ")\n",
    "print(f\"[Exported] CSV files written to: {_graph_export_path}\")\n",
    "\n",
    "# ---- immediately create the *next* empty folder for the next export ----\n",
    "current_export_dir = _make_new_export_dir()\n",
    "print(f\"[Prepared] Next export folder: {current_export_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f908b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created Group Names:\")\n",
    "for i, group_name in enumerate(groups, start=1):\n",
    "    print(f\"[{i:02}] - {group_name}\")\n",
    "\n",
    "\n",
    "def show_model_by_name():\n",
    "    name = input(\"Enter the model variable name (e.g., Grouped Name): \").strip()\n",
    "    obj = globals().get(name, None)\n",
    "    if obj is None:\n",
    "        raise NameError(f\"No variable named '{name}' found in the global scope.\")\n",
    "    # Optionally: sanity check it's a Topology (skip if not needed)\n",
    "    try:\n",
    "        Topology.Show(obj)\n",
    "        print(f\"Displayed: {name}\")\n",
    "    except Exception as e:\n",
    "        raise TypeError(f\"'{name}' is not a valid TopologicPy Topology or cannot be shown. Details: {e}\")\n",
    "\n",
    "show_model_by_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== GRAPH COMPARISON UTILITIES =====================\n",
    "import os, csv, re\n",
    "from datetime import datetime\n",
    "\n",
    "# Reuse your break helper if present; else define here\n",
    "try:\n",
    "    _is_break\n",
    "except NameError:\n",
    "    def _is_break(s: str) -> bool:\n",
    "        return isinstance(s, str) and s.strip().lower() in {\"break\", \"q\", \"quit\", \"exit\"}\n",
    "\n",
    "# Root for exports (same as you used for Graph.ExportToCSV)\n",
    "try:\n",
    "    EXPORT_ROOT\n",
    "except NameError:\n",
    "    EXPORT_ROOT = r\"C:\\Users\\user\\Downloads\\Topologic_Graph_Exports\"\n",
    "\n",
    "def _ensure_dir(path: str) -> str:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def _list_export_runs(root: str):\n",
    "    if not os.path.isdir(root):\n",
    "        return []\n",
    "    # Only directories; ignore comparison_results\n",
    "    runs = [d for d in os.listdir(root)\n",
    "            if os.path.isdir(os.path.join(root, d)) and d.lower() != \"comparison_results\"]\n",
    "    runs.sort()\n",
    "    return runs\n",
    "\n",
    "def _find_csvs(folder: str):\n",
    "    \"\"\"Heuristically pick node/edge CSVs from a folder.\"\"\"\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith(\".csv\")]\n",
    "    nodes_csv = None\n",
    "    edges_csv = None\n",
    "    # Prefer explicit names\n",
    "    for f in files:\n",
    "        fl = f.lower()\n",
    "        if \"node\" in fl and nodes_csv is None:\n",
    "            nodes_csv = f\n",
    "        if \"edge\" in fl and edges_csv is None:\n",
    "            edges_csv = f\n",
    "    # Fallback: first/second csv if ambiguous\n",
    "    if nodes_csv is None and files:\n",
    "        nodes_csv = files[0]\n",
    "    if edges_csv is None and len(files) >= 2:\n",
    "        # pick the other one\n",
    "        others = [f for f in files if f != nodes_csv]\n",
    "        edges_csv = others[0] if others else None\n",
    "    return (os.path.join(folder, nodes_csv) if nodes_csv else None,\n",
    "            os.path.join(folder, edges_csv) if edges_csv else None)\n",
    "\n",
    "def _read_csv(path: str):\n",
    "    with open(path, newline=\"\", encoding=\"utf-8-sig\") as fp:\n",
    "        rdr = csv.DictReader(fp)\n",
    "        rows = [dict(r) for r in rdr]\n",
    "        headers = rdr.fieldnames or []\n",
    "    return headers, rows\n",
    "\n",
    "def _detect_node_id_header(headers):\n",
    "    candidates = [\n",
    "        \"id\", \"node_id\", \"node\", \"label_id\", \"node_label\", \"n\", \"index\", \"uid\"\n",
    "    ]\n",
    "    hlow = [h.lower() for h in headers]\n",
    "    for c in candidates:\n",
    "        if c in hlow:\n",
    "            return headers[hlow.index(c)]\n",
    "    # fallback = first column\n",
    "    return headers[0] if headers else None\n",
    "\n",
    "def _detect_edge_uv_headers(headers):\n",
    "    \"\"\"Return (u_col, v_col, label_col or None).\"\"\"\n",
    "    cand_u = [\"u\",\"source\",\"from\",\"start\",\"node_u\",\"id1\",\"a\",\"src\",\"tail\",\"head_1\",\"s\"]\n",
    "    cand_v = [\"v\",\"target\",\"to\",\"end\",\"node_v\",\"id2\",\"b\",\"dst\",\"head\",\"head_2\",\"t\"]\n",
    "    cand_lbl = [\"label_id\",\"label\",\"edge_label\",\"type\",\"class\"]\n",
    "    hlow = [h.lower() for h in headers]\n",
    "\n",
    "    u_col = v_col = lbl_col = None\n",
    "\n",
    "    for c in cand_u:\n",
    "        if c in hlow:\n",
    "            u_col = headers[hlow.index(c)]\n",
    "            break\n",
    "    for c in cand_v:\n",
    "        if c in hlow:\n",
    "            v_col = headers[hlow.index(c)]\n",
    "            break\n",
    "    for c in cand_lbl:\n",
    "        if c in hlow:\n",
    "            lbl_col = headers[hlow.index(c)]\n",
    "            break\n",
    "\n",
    "    # fallback to first two columns if not found\n",
    "    if (u_col is None or v_col is None) and len(headers) >= 2:\n",
    "        u_col = u_col or headers[0]\n",
    "        v_col = v_col or headers[1]\n",
    "\n",
    "    return u_col, v_col, lbl_col\n",
    "\n",
    "def _safe_id(x):\n",
    "    \"\"\"Normalise IDs to strings without trailing/leading spaces for stable comparison.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def _load_graph_from_folder(folder: str):\n",
    "    \"\"\"Load graph as simple python sets/dicts from a CSV folder.\"\"\"\n",
    "    nodes_csv, edges_csv = _find_csvs(folder)\n",
    "    if not nodes_csv or not os.path.exists(nodes_csv):\n",
    "        raise FileNotFoundError(f\"No node CSV found in: {folder}\")\n",
    "    if not edges_csv or not os.path.exists(edges_csv):\n",
    "        raise FileNotFoundError(f\"No edge CSV found in: {folder}\")\n",
    "\n",
    "    # Nodes\n",
    "    n_headers, n_rows = _read_csv(nodes_csv)\n",
    "    nid_col = _detect_node_id_header(n_headers)\n",
    "    if nid_col is None:\n",
    "        raise RuntimeError(f\"Cannot detect node id column in: {nodes_csv}\")\n",
    "\n",
    "    nodes = set()\n",
    "    node_attrs = {}  # id -> {attrs…}\n",
    "    for r in n_rows:\n",
    "        nid = _safe_id(r.get(nid_col))\n",
    "        if not nid:\n",
    "            continue\n",
    "        nodes.add(nid)\n",
    "        # store all attributes\n",
    "        node_attrs[nid] = {k: r.get(k) for k in n_headers}\n",
    "\n",
    "    # Edges\n",
    "    e_headers, e_rows = _read_csv(edges_csv)\n",
    "    u_col, v_col, e_label_col = _detect_edge_uv_headers(e_headers)\n",
    "    if u_col is None or v_col is None:\n",
    "        raise RuntimeError(f\"Cannot detect edge endpoints in: {edges_csv}\")\n",
    "\n",
    "    # treat edges as undirected for set operations (sort endpoints)\n",
    "    def _edge_key(a, b):\n",
    "        a, b = _safe_id(a), _safe_id(b)\n",
    "        return tuple(sorted((a, b)))\n",
    "\n",
    "    edges = set()\n",
    "    edge_attrs = {}  # (u,v) -> {attrs…}\n",
    "    for r in e_rows:\n",
    "        u = r.get(u_col)\n",
    "        v = r.get(v_col)\n",
    "        if u is None or v is None:\n",
    "            continue\n",
    "        ek = _edge_key(u, v)\n",
    "        edges.add(ek)\n",
    "        edge_attrs[ek] = {k: r.get(k) for k in e_headers}\n",
    "\n",
    "    return {\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"node_attrs\": node_attrs,\n",
    "        \"edge_attrs\": edge_attrs,\n",
    "        \"nid_col\": nid_col,\n",
    "        \"u_col\": u_col,\n",
    "        \"v_col\": v_col,\n",
    "        \"node_headers\": n_headers,\n",
    "        \"edge_headers\": e_headers\n",
    "    }\n",
    "\n",
    "def _compare_graphs(GA, GB):\n",
    "    # Node/edge sets\n",
    "    A_nodes, B_nodes = GA[\"nodes\"], GB[\"nodes\"]\n",
    "    A_edges, B_edges = GA[\"edges\"], GB[\"edges\"]\n",
    "\n",
    "    common_nodes = A_nodes & B_nodes\n",
    "    onlyA_nodes = A_nodes - B_nodes\n",
    "    onlyB_nodes = B_nodes - A_nodes\n",
    "\n",
    "    common_edges = A_edges & B_edges\n",
    "    onlyA_edges = A_edges - B_edges\n",
    "    onlyB_edges = B_edges - A_edges\n",
    "\n",
    "    # Attribute diffs for nodes present in both\n",
    "    node_attr_diffs = []\n",
    "    for nid in common_nodes:\n",
    "        a = GA[\"node_attrs\"].get(nid, {})\n",
    "        b = GB[\"node_attrs\"].get(nid, {})\n",
    "        # Compare intersection of keys\n",
    "        shared_keys = set(a.keys()) | set(b.keys())\n",
    "        diffs = {k: (a.get(k), b.get(k)) for k in shared_keys if (a.get(k) != b.get(k))}\n",
    "        if diffs:\n",
    "            node_attr_diffs.append({\"node_id\": nid, \"diffs\": diffs})\n",
    "\n",
    "    # Attribute diffs for edges present in both\n",
    "    edge_attr_diffs = []\n",
    "    for e in common_edges:\n",
    "        a = GA[\"edge_attrs\"].get(e, {})\n",
    "        b = GB[\"edge_attrs\"].get(e, {})\n",
    "        shared_keys = set(a.keys()) | set(b.keys())\n",
    "        diffs = {k: (a.get(k), b.get(k)) for k in shared_keys if (a.get(k) != b.get(k))}\n",
    "        if diffs:\n",
    "            edge_attr_diffs.append({\"edge\": e, \"diffs\": diffs})\n",
    "\n",
    "    return {\n",
    "        \"common_nodes\": common_nodes,\n",
    "        \"onlyA_nodes\": onlyA_nodes,\n",
    "        \"onlyB_nodes\": onlyB_nodes,\n",
    "        \"common_edges\": common_edges,\n",
    "        \"onlyA_edges\": onlyA_edges,\n",
    "        \"onlyB_edges\": onlyB_edges,\n",
    "        \"node_attr_diffs\": node_attr_diffs,\n",
    "        \"edge_attr_diffs\": edge_attr_diffs\n",
    "    }\n",
    "\n",
    "def _write_csv(path, headers, rows):\n",
    "    _ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        wr = csv.DictWriter(fp, fieldnames=headers)\n",
    "        wr.writeheader()\n",
    "        for r in rows:\n",
    "            wr.writerow(r)\n",
    "\n",
    "def _save_comparison(root, runA, runB, GA, GB, cmpres):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_root = _ensure_dir(os.path.join(root, \"comparison_results\", f\"{runA}_VS_{runB}_{ts}\"))\n",
    "\n",
    "    # Nodes\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, \"nodes_common.csv\"),\n",
    "        [\"node_id\"],\n",
    "        [{\"node_id\": n} for n in sorted(cmpres[\"common_nodes\"])]\n",
    "    )\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, f\"nodes_only_{runA}.csv\"),\n",
    "        [\"node_id\"],\n",
    "        [{\"node_id\": n} for n in sorted(cmpres[\"onlyA_nodes\"])]\n",
    "    )\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, f\"nodes_only_{runB}.csv\"),\n",
    "        [\"node_id\"],\n",
    "        [{\"node_id\": n} for n in sorted(cmpres[\"onlyB_nodes\"])]\n",
    "    )\n",
    "\n",
    "    # Edges\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, \"edges_common.csv\"),\n",
    "        [\"u\",\"v\"],\n",
    "        [{\"u\": u, \"v\": v} for (u,v) in sorted(cmpres[\"common_edges\"])]\n",
    "    )\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, f\"edges_only_{runA}.csv\"),\n",
    "        [\"u\",\"v\"],\n",
    "        [{\"u\": u, \"v\": v} for (u,v) in sorted(cmpres[\"onlyA_edges\"])]\n",
    "    )\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, f\"edges_only_{runB}.csv\"),\n",
    "        [\"u\",\"v\"],\n",
    "        [{\"u\": u, \"v\": v} for (u,v) in sorted(cmpres[\"onlyB_edges\"])]\n",
    "    )\n",
    "\n",
    "    # Attribute diffs (flatten)\n",
    "    def _flatten_attr_diffs(items, node_mode=True):\n",
    "        flat = []\n",
    "        if node_mode:\n",
    "            for it in items:\n",
    "                nid = it[\"node_id\"]\n",
    "                for k, (av, bv) in it[\"diffs\"].items():\n",
    "                    flat.append({\"node_id\": nid, \"attr\": k, \"A_value\": av, \"B_value\": bv})\n",
    "        else:\n",
    "            for it in items:\n",
    "                (u,v) = it[\"edge\"]\n",
    "                for k, (av, bv) in it[\"diffs\"].items():\n",
    "                    flat.append({\"u\": u, \"v\": v, \"attr\": k, \"A_value\": av, \"B_value\": bv})\n",
    "        return flat\n",
    "\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, \"node_attribute_differences.csv\"),\n",
    "        [\"node_id\",\"attr\",\"A_value\",\"B_value\"],\n",
    "        _flatten_attr_diffs(cmpres[\"node_attr_diffs\"], node_mode=True)\n",
    "    )\n",
    "    _write_csv(\n",
    "        os.path.join(out_root, \"edge_attribute_differences.csv\"),\n",
    "        [\"u\",\"v\",\"attr\",\"A_value\",\"B_value\"],\n",
    "        _flatten_attr_diffs(cmpres[\"edge_attr_diffs\"], node_mode=False)\n",
    "    )\n",
    "\n",
    "    # Text report\n",
    "    report_path = os.path.join(out_root, \"comparison_report.txt\")\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(f\"Graph Comparison Report\\n\")\n",
    "        fp.write(f\"Run A: {runA}\\nRun B: {runB}\\n\")\n",
    "        fp.write(f\"Export root: {root}\\nGenerated: {ts}\\n\\n\")\n",
    "\n",
    "        fp.write(\"COUNTS\\n\")\n",
    "        fp.write(f\"- Nodes A: {len(GA['nodes'])}, Nodes B: {len(GB['nodes'])}\\n\")\n",
    "        fp.write(f\"- Common nodes: {len(cmpres['common_nodes'])}\\n\")\n",
    "        fp.write(f\"- A-only nodes: {len(cmpres['onlyA_nodes'])}\\n\")\n",
    "        fp.write(f\"- B-only nodes: {len(cmpres['onlyB_nodes'])}\\n\\n\")\n",
    "        fp.write(f\"- Edges A: {len(GA['edges'])}, Edges B: {len(GB['edges'])}\\n\")\n",
    "        fp.write(f\"- Common edges: {len(cmpres['common_edges'])}\\n\")\n",
    "        fp.write(f\"- A-only edges: {len(cmpres['onlyA_edges'])}\\n\")\n",
    "        fp.write(f\"- B-only edges: {len(cmpres['onlyB_edges'])}\\n\\n\")\n",
    "\n",
    "        fp.write(\"ATTRIBUTE DIFFERENCES\\n\")\n",
    "        fp.write(f\"- Nodes with attribute diffs: {len(cmpres['node_attr_diffs'])}\\n\")\n",
    "        fp.write(f\"- Edges with attribute diffs: {len(cmpres['edge_attr_diffs'])}\\n\")\n",
    "\n",
    "    print(f\"[Saved] Comparison CSVs & report at:\\n  {out_root}\")\n",
    "    return out_root\n",
    "\n",
    "def compare_two_exported_runs():\n",
    "    \"\"\"Interactive: pick two folders under EXPORT_ROOT and compare them.\"\"\"\n",
    "    runs = _list_export_runs(EXPORT_ROOT)\n",
    "    if not runs:\n",
    "        print(f\"No export runs found under: {EXPORT_ROOT}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nAvailable export runs:\")\n",
    "    for i, r in enumerate(runs, 1):\n",
    "        print(f\"  {i}. {r}\")\n",
    "\n",
    "    # Select A\n",
    "    rawA = input(\"\\nSelect RUN A (number) or 'break' to stop: \").strip()\n",
    "    if _is_break(rawA):\n",
    "        print(\"Stopping by user request.\")\n",
    "        return\n",
    "    # Select B\n",
    "    rawB = input(\"Select RUN B (number) or 'break' to stop: \").strip()\n",
    "    if _is_break(rawB):\n",
    "        print(\"Stopping by user request.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        iA = int(rawA); iB = int(rawB)\n",
    "        assert 1 <= iA <= len(runs) and 1 <= iB <= len(runs) and iA != iB\n",
    "    except Exception:\n",
    "        print(\"Invalid selection. Please run again.\")\n",
    "        return\n",
    "\n",
    "    runA, runB = runs[iA-1], runs[iB-1]\n",
    "    folderA = os.path.join(EXPORT_ROOT, runA)\n",
    "    folderB = os.path.join(EXPORT_ROOT, runB)\n",
    "\n",
    "    print(f\"\\n[Loading] RUN A: {folderA}\")\n",
    "    GA = _load_graph_from_folder(folderA)\n",
    "    print(f\"[Loading] RUN B: {folderB}\")\n",
    "    GB = _load_graph_from_folder(folderB)\n",
    "\n",
    "    print(\"[Comparing] Computing intersections & differences…\")\n",
    "    cmpres = _compare_graphs(GA, GB)\n",
    "\n",
    "    # Quick console summary\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Nodes A/B: {len(GA['nodes'])}/{len(GB['nodes'])}\")\n",
    "    print(f\"Common nodes: {len(cmpres['common_nodes'])} | A-only: {len(cmpres['onlyA_nodes'])} | B-only: {len(cmpres['onlyB_nodes'])}\")\n",
    "    print(f\"Edges A/B: {len(GA['edges'])}/{len(GB['edges'])}\")\n",
    "    print(f\"Common edges: {len(cmpres['common_edges'])} | A-only: {len(cmpres['onlyA_edges'])} | B-only: {len(cmpres['onlyB_edges'])}\")\n",
    "    print(f\"Node attr diffs: {len(cmpres['node_attr_diffs'])} | Edge attr diffs: {len(cmpres['edge_attr_diffs'])}\")\n",
    "\n",
    "    # Save full artefacts\n",
    "    out_dir = _save_comparison(EXPORT_ROOT, runA, runB, GA, GB, cmpres)\n",
    "\n",
    "    # Optional: show a few examples inline\n",
    "    def _peek(s, n=10): \n",
    "        return list(s)[:min(len(s), n)]\n",
    "    print(\"\\nExamples (first few):\")\n",
    "    print(\"  A-only nodes:\", _peek(cmpres[\"onlyA_nodes\"]))\n",
    "    print(\"  B-only nodes:\", _peek(cmpres[\"onlyB_nodes\"]))\n",
    "    print(\"  A-only edges:\", _peek(cmpres[\"onlyA_edges\"]))\n",
    "    print(\"  B-only edges:\", _peek(cmpres[\"onlyB_edges\"]))\n",
    "    print(\"\\nDone.\")\n",
    "    return out_dir\n",
    "\n",
    "# ===================== HOW TO RUN =====================\n",
    "# Whenever you want to compare two exported runs, call:\n",
    "# compare_two_exported_runs()\n",
    "#\n",
    "# Tip: If you want to run automatically after a fresh export, just call it\n",
    "# right after Graph.ExportToCSV(...) in your pipeline.\n",
    "compare_two_exported_runs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
